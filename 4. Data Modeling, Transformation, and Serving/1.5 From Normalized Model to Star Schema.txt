As a data engineer, there is a good chance
you'll need to transform data that's stored in a normalized
schema into a star schema. For example, you might need to extract
normalized data that's stored in a relational database and
model the data into a star schema so it's easier to query before loading it
into department-specific data marts. So let's walk through an example where
we convert normalized data that's in third normal form into a star schema. Here's the Diagram of the normalized
data we obtained in an earlier video by applying the normalization stages. It consists of four tables. Customers, orders placed by the customers,
the items that make up each order, and the characteristics of each item. I'll add another table here to represent
the stores where each order was placed. In this diagram, PK means primary key and
FK means foreign key. Let's say you're tasked with modeling
this data into a star schema to serve to the data analyst in your company. We'll follow the four key steps Kimball
proposed for designing a star schema. First, you need to understand
the needs of the business. This helps you identify which business
events or processes you want to model in the fact tables, and
it'll help you declare the grain, meaning the level of detail you want
each row in the fact table to represent. Then you can identify the business
measurements or facts that are generated by the business processes and
are associated with the grain, as well as choose the dimensions to
provide the needed context for the facts. Let's say you did some requirements
gathering by talking to a data analyst, and learned that they are interested in
analyzing the sales data to understand which products are selling in
which stores on a given day. if there are any differences
in sales between stores, and which product brands are most popular. From this, you've determined
that the business process that needs to be modeled is
the company's sales transactions. So you could decide to
represent the total sales transactions on a particular day
in each row of the fact table. Or you could represent
a single sales transaction, or even an individual product item
in a sales transaction in each row. When deciding on the grain,
like I mentioned earlier, I encourage you to choose the atomic
grain to capture the lowest level of detail of the sales transactions. This way, you'll be able to keep your
system flexible enough to handle unpredicted user questions in the future. So let's declare the grain to be
an individual product item in a sales transaction. Next, you want to select the dimensions. Since the data analyst is
interested in analyzing the sales with respect to the stores,
dates, and brands, you can have one dimension table
that represents the stores, another dimension for the item's features,
and another for the date dimension. So let's write a SQL statement to
create the stores dimension table. You want to select the store_id,
store_name, store_city, and store_zipcode from the stores table, and you also
need a primary key for this table. Like I mentioned in an earlier video, you
typically want to generate surrogate keys and use them as the primary keys for
your dimension and fact tables. This guarantee is that each
row in your store schema can be uniquely identified by the primary
key of the fact and dimension tables, regardless of changes that might
happen to your source systems. To generate the surrogate key for
the stores dimension table, you can create a sequence of integers starting from 1,
and assign one integer to each store. Or you can use a hash function that
takes in the natural primary key and generates a unique surrogate key for
each store. Popular database management
systems such as PostgreSQL and MySQL support several hash functions. For example, MD5 is a hash function that
encodes a string into a hash output. So assuming that the store_id from
the production database is a string, I can apply the MD5 hash function to the
store_id to generate a surrogate key for each row, and
I'll label the surrogate key as store_key. If the storeid is actually
an integer value, then you'll need to first
convert it to a string. So with this SQL statement you'll end
up with this store's dimension table. Notice that it contains both the surrogate
store key and the natural store_id for ease of reference and
interpretability purposes. Next, let's create the items dimension
table by selecting the SKU, name, and brand from the items table. Then for the primary key, you can
apply the MD5 hash function to SKU, assuming that SKU is a string, and
then label the surrogate key as item_key. You'll end up with this
items dimension table. Finally, let's create
the date dimension table. The idea behind the date
dimension is that for each date you can create columns that
specify the corresponding day of the week, month, quarter, and
year like you see here. This will help the data analyst answer
questions like what are the total sales in the first quarter of 2022? What products are most
popular on the weekends? To create the date dimension table, you can generate a series of sequential
dates covering a desired period of time. Here's one way of generating a series
of daily dates in PostgreSQL. First, you'll generate a series
of dates from January 1, 2020 all the way to January 1, 2025. Then you'll extract the day of the week,
month, quarter, and year from each date. So that takes care of
the three dimension tables. Finally, let's create the fact tables. Each row in the fact table must represent
a product within a sales transaction. The facts associated with each
product are the quantity sold, which you can get from
the order items table, and the price of each item,
which you can get from the items table. Additionally, the fact tables must
contain the foreign keys that connect it to the dimension tables. In this case, those are the store_key,
item_key, and date_ key, which are the surrogate keys created for
each dimension table. And of course, the fact table must contain a primary
key to uniquely identify each row. You could create a composite
key consisting of order_id and item_line_number, but the better
practice is to generate a surrogate key from the combination of
these two natural keys. So the fact_order_items table
should look something like this. Now let's write the SQL query
to create this fact table. For the primary key, you'll select
the order_id and item_line_number from the order items table, and then
concatenate them so you can apply the MD5 hash function to the concatenated
string to generate the surrogate key. You'll label this as fact_order_key. You'll also include the order_id and item_line_number natural keys
in the fact table for reference. Then you'll join the orders table and
then the items table so you can create the rest of the attributes. For the foreign keys, let's apply the hash
function to the store_id from the orders table to create the store_key that
links to the store dimension table. Then let's apply the hash function to
the item SKU from the order_items table to create the item key that links
to the item dimensions table. And finally, let's select
the order_date from the orders table to create the date key that links
to the date dimension table. And don't forget,
we need to add the two facts. So let's select the item quantity
from the order items table and the price from the items table. Here's the Diagram of the star
schema we just created. Since an item in an order is purchased on
a particular date, each row in the order items fact table can only be associated
with one row in the date dimension table. So the relationship from the order items
fact table to the date dimension table is one-to-one, and
is denoted by this symbol here. On the other hand, a particular
date can be associated with no orders if no one purchased
anything on that date, or it can be associated with many orders
if multiple orders were made that day. So the relationship from the date
dimension table to the order items fact table is zero or one- to-many, or
more commonly known as one-to-many. Similarly, an item in an order is
purchased at a given store and corresponds to a single item. So each row in the fact table is
associated with one row in the store dimension table and
one row in the item dimension table. And the relationships from the fact table
to these dimension tables are one-to-one. But any store and any item can be
associated with more than one order. So the relationships from these dimension
tables to the fact table are one-to-many. So that's how you can convert
a normalized model into a star schema. If you want more practice, I have included
another modeling example in the reading item that follows this video
in the last lab of this week, you'll also get a chance to model
normalized data into a star schema. While you can write your own code to
transform the data like you've done in the previous labs with AWS Glue,
in this lab you'll use a popular data transformation tool called DBT,
which helps you model your data by abstracting away a lot of the heavy
lifting with writing pure SQL code. DBT allows you to connect to your
data warehouse and then transform and validate your data within
the data warehouse itself. It treats a modeling process
as a transformation task and generates a SQL code behind
the scenes to transform your data. Although DBT simplifies
the transformation step for you, it can only connect to a single target, meaning you can't use DBT to join
together data from different sources, and you can't move the data to another target
system after it's been transformed. To join the data from multiple sources, you need to first bring the data
inside the same target system. AWS Glue, on the other hand, allows
you to connect two different sources, apply transformations, and
store the process data somewhere else. So if you need to perform transformations
on data that will be moved around, you should choose AWS glue or
a similar ingestion tool over DBT. So after you get more practice
with dimensional modeling and the reading item that follows, join me in an optional video where I speak
with Drew Bannon, the co-founder of DBT. Then after that, we'll take a look at another modeling
technique called Data Vault. I'll see you there.