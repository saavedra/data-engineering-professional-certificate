To address the shortcomings
of Hadoop MapReduce, researchers at UC Berkeley started
the Spark project back in 2009. The goal was to come up with a distributed
framework that borrows ideas from MapReduce, but is simpler and faster. Supporting in memory storage for
intermediate results and interactive processing of the data. Spark has since evolved to support
streaming processing as well as machine learning and graphing libraries. And as new features are continuously
being developed and added to this framework by
the Apache Spark community, I think it's critical that you
gain some experience with Spark. Unlike Hadoop,
which integrates compute and persistent storage via the Hadoop
distributed file system, Spark is just a computing engine designed
for processing distributed large datasets. With Spark,
you can perform parallel computations and retain intermediate results in memory,
which limits disk IO interactions, enabling significantly faster
computation than Hadoop MapReduce. You can use Spark locally in
data centers or in the cloud. You can load data from and store the final
results on separate persistent storage systems such as relational or
key value databases, object storage, or even Hadoop file system. Spark provides a unified platform that
allows you to run different types of analytical workloads as self
contained Spark applications on one processing engine. For example, you can use Spark to
perform SQL queries to load data, train and
test machine learning algorithms, and apply streaming transformations on your
data over the same computation engine. You can write your workloads in Python,
Java, Scala, and R using Spark core APIs. Spark also offers built-in
libraries such as Spark SQL for writing SQL queries, ML lib for
machine learning applications. Spark structured streaming for interacting
with real time data, and graphics for graph processing. Beyond these standard libraries, you can
also use external third party libraries published and
maintained by the open source communities. These include connectors that allow you
to connect to a variety of external data sources and storage systems,
monitor performance, and much more. Let's zoom in on the underlying
components of a Spark Application and see how they work together
to process your code. A spark application consists
of a cluster of nodes. It has a driver node, which is the central
controller of a Spark application. A cluster manager node, which communicates
with the driver to allocate computational and memory resources across a cluster and
manage these resources. And a set of worker nodes where each node
contains a spark executor that executes tasks assigned to them by the driver. Spark applies a partitioning scheme
to break up the data into partitions when loading it from disk, and allocates
to each spark executor the partitions that are closest to it in the network. So each executor's cpu core gets
a partition of data to work on. When you write a spark application,
you start by instantiating a spark session object that represents a single, unified
entry point to all Spark's functionality. Through this entry, you can define data
frames, read data from sources, and perform SQL queries. The driver node translates
the instructions you wrote, which could be in python, scala, or
another language, into Spark jobs, which will be executed one by
one based on the job's priority. To do this, the driver transforms each
job into a sequence of stages and represents these stages as a DAG. For example, this job here has three
stages and is represented with this dag. So each DAG is sort of like the execution
plan for the corresponding job. Each stage is further broken down into
tasks written in smart code that can run in parallel. Here, stage 1 has four tasks that
run in parallel, and stages 2 and 3 each have three tasks that
can also run in parallel. You'll run stages that have
shared dependencies serially and run those without
dependencies in parallel. For example, in this stack, stages 2 and
3 depend on the results of stage 1, so they can't start until stage 1 is done. But stages 2 and 3 don't have any shared
dependencies and can be run in parallel. So to execute this job,
Spark will start with stage 1 and run all four tasks in parallel. Once these tasks are done, stage two and
three will both start, and the three tasks in each
stage will run in parallel. Returning to our spark application once
the DAG execution plan is developed, the driver communicates with the cluster
manager to request computational and memory resources for the executors. Each task is assigned to a single
core within an executor, and each executor works on
a single data partition. The executor executes the task and
communicates the computation results back to the driver node, and finally, the
driver node aggregates these computations and returns the results back to you. When interacting with Spark, you don't need to worry about
all these underlying details. So whether you write your application
using your favorite programming language or using any of the standard libraries. Behind the scenes, your code will
be decomposed into tasks and assigned across the Spark executors. There's a lot we can cover with Spark, but I want to focus on PySpark,
which is the Python API for Apache. Spark Pyspark supports all Spark features,
including Spark SQL, Spark dataframes, machine learning,
and structured streaming. In the lab, you'll work with structured
data using Spark dataframes and Spark SQL. So in the next video I'll
show you how to create and work with Spark dataframes in the video
after that, we'll go over Spark SQL.