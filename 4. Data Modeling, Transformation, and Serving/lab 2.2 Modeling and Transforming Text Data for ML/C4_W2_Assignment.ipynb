{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Modeling and Transforming Text Data for ML\n",
        "\n",
        "In this assignment, you will work with the Amazon Reviews dataset to generate a training dataset for an ML use case. The purpose of this lab is to apply feature engineering to process the numerical and categorical features of the raw JSON files, and to transform the text reviews and product information into text embeddings. You will finally store the generated features in a provided Postgres database, which will serve as a vector database."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Table of Contents\n",
        "\n",
        "- [ 1 - Introduction](#1)\n",
        "- [ 2 - Source Data Exploration](#2)\n",
        "- [ 3 - Creating Functions to Process Data](#3)\n",
        "  - [ 3.1 - Process the Reviews Dataset](#3.1)\n",
        "  - [ 3.2 - Process the Metatada Dataset](#3.2)\n",
        "  - [ 3.3 - Process the Textual Features](#3.3)\n",
        "  - [ 3.4 - Process the Numerical Features](#3.4)\n",
        "  - [ 3.5 - Process the Categorical Features](#3.5)\n",
        "- [ 4 - Split Data and Create Text Embeddings](#4)\n",
        "  - [ 4.1 - Split Data](#4.1)\n",
        "  - [ 4.2 - Create Text Embeddings](#4.2)\n",
        "- [ 5 - Upload Files for Grading](#5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the required libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import datetime as dt\n",
        "import gzip\n",
        "import json\n",
        "import math\n",
        "import requests\n",
        "import time\n",
        "import os\n",
        "import subprocess\n",
        "from typing import Dict, List, Union\n",
        "\n",
        "\n",
        "import boto3\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import psycopg2 \n",
        "import smart_open\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "from pgvector.psycopg2 import register_vector\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, StandardScaler, KBinsDiscretizer\n",
        "\n",
        "pd.set_option('display.max_columns', 30)\n",
        "\n",
        "LAB_PREFIX='de-c4w2a1'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name='1'></a>\n",
        "## 1 - Introduction\n",
        "\n",
        "Imagine you are employed as a Data Engineer at a prominent e-commerce retailer. The Machine Learning (ML) team has initiated a new research project and obtained a dataset comprising Amazon Reviews for different products. They have requested you to build a pipeline to refine the raw JSON data into structured datasets suitable for training ML models. To start the development, they have provided you with two sample files from the original dataset to validate the logic and develop an initial pipeline prototype within this notebook. Additionally, the Data Analytics team has requested that you generate embeddings from the reviews and product texts and then store the vectors in a vector database for future analysis; for this purpose, the ML team has enabled an API that runs a text embedder ML model for you to consume and generate the vectors.\n",
        "\n",
        "The main requirements regarding the datasets are the following:\n",
        "\n",
        "1. Process the textual, categorical and numerical features.\n",
        "2. Generate text embeddings based on the review text and product information (provided from the product description or the product title)\n",
        "3. Divide the original data into three tables:\n",
        "   - Reviews embeddings dataset: it must contain the reviewer ID, product ASIN, the review text and the corresponding embedding vector.\n",
        "   - Product embeddings dataset: it must contain the product ASIN, the product information and the corresponding embedding vector.\n",
        "   - Review metadata dataset: it must contain the remaining features related to the reviews and products for each review from the original data.\n",
        "4. Store the new features in the provisioned RDS Postgres instance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name='2'></a>\n",
        "## 2 - Source Data Exploration\n",
        "\n",
        "The dataset is comprised of two compressed JSON files, one with the reviews and one with the metadata of the reviewed products. You've already worked with this dataset in C3W2 lab. Here is an example of a review:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"reviewerID\": \"A2SUAM1J3GNN3B\",\n",
        "  \"asin\": \"0000013714\",\n",
        "  \"reviewerName\": \"J. McDonald\",\n",
        "  \"helpful\": [2, 3],\n",
        "  \"reviewText\": \"I bought this for my husband who plays the piano.  He is having a wonderful time playing these old hymns.  The music  is at times hard to read because we think the book was published for singing from more than playing from.  Great purchase though!\",\n",
        "  \"overall\": 5.0,\n",
        "  \"summary\": \"Heavenly Highway Hymns\",\n",
        "  \"unixReviewTime\": 1252800000,\n",
        "  \"reviewTime\": \"09 13, 2009\"\n",
        "}\n",
        "```\n",
        "\n",
        "Here is the description of the fields:\n",
        "\n",
        "- `reviewerID` - ID of the reviewer, e.g. A2SUAM1J3GNN3B\n",
        "- `asin` - ID of the product, e.g. 0000013714\n",
        "- `reviewerName` - name of the reviewer\n",
        "- `helpful` - helpfulness rating of the review, e.g. 2/3\n",
        "- `reviewText` - text of the review\n",
        "- `overall` - rating of the product\n",
        "- `summary` - summary of the review\n",
        "- `unixReviewTime` - time of the review (unix time)\n",
        "- `reviewTime` - time of the review (raw)\n",
        "\n",
        "And this is an example of the review metadata:\n",
        "```json\n",
        "{\n",
        "  \"asin\": \"0641843224\",\n",
        "  \"description\": \"Set your phasers to stun and prepare for a warp speed ride through the most memorable vocabulary from the sci-fi/fantasy genre.\",\n",
        "  \"title\": \"McNeill Designs YBS Sci-fi/Fantasy Add-on Deck\", \n",
        "  \"price\": 5.19,  \n",
        "  \"imUrl\": \"http://ecx.images-amazon.com/images/I/418t9AN9hiL._SY300_.jpg\", \n",
        "  \"related\": \n",
        "  {\n",
        "    \"also_bought\": [\"B000EVLZ9U\", \"0641843208\", \"0641843216\", \"0641843267\", \"1450751210\", \"0641843232\", \"B00ALQFYGI\", \"B004G7B3NQ\", \"B002PDM288\", \"B009ZNJZV8\", \"B009YG928W\", \"B0063NC3N0\"], \n",
        "    \"also_viewed\": [\"B000EVLZ9U\", \"1450751210\", \"0641843208\", \"0641843267\", \"0641843232\", \"0641843216\", \"B003EIK136\", \"B004G7B3NQ\", \"B003N2Q5JC\"], \n",
        "    \"bought_together\": [\"B000EVLZ9U\"]\n",
        "  },\n",
        "  \"salesRank\": {\"Toys & Games\": 154868}, \n",
        "  \"brand\": \"McNeill Designs\", \n",
        "  \"categories\": [[\"Toys & Games\", \"Games\", \"Card Games\"]]\n",
        "}\n",
        "```\n",
        "\n",
        "With the following fields:\n",
        "\n",
        "- `asin` - ID of the product, e.g. 0000031852\n",
        "- `description` - Description of the product\n",
        "- `title` - name of the product\n",
        "- `price` - price in US dollars (at time of crawl)\n",
        "- `imUrl` - url of the product image\n",
        "- `related` - related products (`also_bought`, `also_viewed`, `bought_together`)\n",
        "- `salesRank` - sales rank information\n",
        "- `brand` - brand name\n",
        "- `categories` - list of categories the product belongs to"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2.1. The ML team has provided the dataset JSON files in an S3 bucket. Go to the AWS console and click on the upper right part, where your username appears. Copy the **Account ID**. In the code below, set the variable `BUCKET_NAME` to your account ID by replacing `<AWS-ACCOUNT-ID>` placeholder with the Account ID that you copied. The Account ID should contain only numbers without hyphens between them (e.g. 123412341234, not 1234-1234-1234).\n",
        "\n",
        "Go to **CloudFormation** in the AWS console. You will see two stacks deployed, one associated with your Cloud9 environment (name with prefix `aws-cloud9`) and another named with an alphanumeric ID. Click on the alphanumeric ID stack and search for the **Outputs** tab. You will see the key `MlModelDNS`, copy the corresponding **Value** and replace with it the placeholder `<ML_MODEL_ENDPOINT_URL>` in the cell below. Please, be careful not to remove the `http://` part and only replace the placeholder with the value you get from CloudFormation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "BUCKET_NAME = 'de-c4w2a1-<AWS-ACCOUNT-ID>-us-east-1-data-lake'\n",
        "ENDPOINT_URL = 'http://<ML_MODEL_ENDPOINT_URL>/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2.2. Open the `./src/env` file and replace the placeholder `<RDS-ENDPOINT>` with the `PostgresEndpoint` value from the CloudFormation outputs. Save changes. Run the following cell to load the connection credentials that will be used later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "load_dotenv('./src/env', override=True)\n",
        "\n",
        "DBHOST = os.getenv('DBHOST')\n",
        "DBPORT = os.getenv('DBPORT')\n",
        "DBNAME = os.getenv('DBNAME')\n",
        "DBUSER = os.getenv('DBUSER')\n",
        "DBPASSWORD = os.getenv('DBPASSWORD')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2.3. Explore the samples of the dataset. You are provided with a function to load the data into memory and explore it. Take a look at the function defined below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def read_data_sample(bucket_name: str, s3_file_key: str) -> pd.DataFrame:\n",
        "    \"\"\"Reads review sample dataset\n",
        "\n",
        "    Args:\n",
        "        bucket_name (str): Bucket name\n",
        "        s3_file_key (str): Dataset s3 key location\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Read dataframe\n",
        "    \"\"\"\n",
        "    s3_client = boto3.client('s3')\n",
        "    source_uri = f's3://{bucket_name}/{s3_file_key}'\n",
        "    json_list = []\n",
        "    for json_line in smart_open.open(source_uri, transport_params={'client': s3_client}):\n",
        "        json_list.append(json.loads(json_line))\n",
        "    df = pd.DataFrame(json_list)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2.3.4. Start reading the sample datasets with the `read_data_sample()` method. The key for the main reviews file is `'staging/reviews_Toys_and_Games_sample.json.gz'` and the one for metadata is `'staging/meta_Toys_and_Games_sample.json.gz'`. Also, assign the `bucket_name` values to `BUCKET_NAME`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "exercise": [
          "ex01"
        ],
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "### START CODE HERE ### (2 lines of code)\n",
        "review_sample_df = None(bucket_name=None, s3_file_key='None')\n",
        "metadata_sample_df = None(bucket_name=None, s3_file_key='None')\n",
        "### END CODE HERE ###\n",
        "\n",
        "review_sample_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "exercise": [
          "ex02"
        ],
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "metadata_sample_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In course 3, you created some functions to perform some processing over the reviews and metadata datasets. Let's recreate those functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name='3'></a>\n",
        "## 3 - Creating Functions to Process Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name='3.1'></a>\n",
        "### 3.1 - Process the Reviews Dataset\n",
        "\n",
        "Complete the `process_review()` function to perform some transformations. Please follow the instructions in the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "exercise": [
          "ex03"
        ],
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "def process_review(raw_df: pd.DataFrame) -> pd.DataFrame:    \n",
        "    \"\"\"Transformations steps for Reviews dataset\n",
        "\n",
        "    Args:\n",
        "        raw_df (DataFrame): Raw data loaded in dataframe\n",
        "\n",
        "    Returns:\n",
        "        DataFrame: Returned transformed dataframe\n",
        "    \"\"\"\n",
        "\n",
        "    ### START CODE HERE ### (5 lines of code)\n",
        "    \n",
        "    # Convert the `unixReviewTime` column of the dataframe `raw_df` to date with the `to_datetime()` function\n",
        "    # The timestamp is defined in seconds (use `s` for the `unit` parameter)\n",
        "    raw_df['reviewTime'] = pd.None(None['None'], unit='None')\n",
        "\n",
        "    # Extract the year and month from the `reviewTime`, and save those values in new columns named `year` and `month`\n",
        "    # You can apply `.dt.year` and `.dt.month` methods to `raw_df['reviewTime']` to do that\n",
        "    raw_df['year'] = None['None'].None.None\n",
        "    raw_df['month'] = None['None'].None.None\n",
        "\n",
        "    # Create a new dataframe based on converting the `helpful` column from the `raw_df` into a list with the `to_list()` method\n",
        "    # Set the column names as `helpful_votes` and `total_votes`\n",
        "    df_helpful = pd.DataFrame(None['None'].None(), columns=['None', 'None'])\n",
        "\n",
        "    # With the `pd.concat()` function, concatenate the `raw_df` dataframe with `df_helpful`\n",
        "    # Make sure that you drop the `helpful` column from `raw_df` with the `drop()` method and  set `axis` equal to 1\n",
        "    target_df = pd.None([None.None(columns=['None']), None], axis=None)\n",
        "    \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    target_df['not_helpful_votes'] = target_df['total_votes'] - target_df['helpful_votes']\n",
        "    \n",
        "    return target_df\n",
        "\n",
        "transformed_review_sample_df = process_review(raw_df=review_sample_df)\n",
        "transformed_review_sample_df.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name='3.2'></a>\n",
        "### 3.2 - Process the Metatada Dataset\n",
        "\n",
        "3.2.1. Let's perform some small transformations over the metadata dataset. Follow the instructions in the code to complete the `process_metadata()` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "exercise": [
          "ex04"
        ],
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "def process_metadata(raw_df: pd.DataFrame, cols: list[str]) -> pd.DataFrame:\n",
        "    \"\"\"Function in charge of the transformation of the raw data of the\n",
        "    Reviews Metadata.\n",
        "\n",
        "    Args:\n",
        "        raw_df (DataFrame): Raw data loaded in dataframe\n",
        "        cols (list): List of columns to select\n",
        "        cols_to_clean (list): List of columns \n",
        "\n",
        "    Returns:\n",
        "        DataFrame: Returned transformed dataframe\n",
        "    \"\"\"\n",
        "\n",
        "    ### START CODE HERE ### (6 lines of code)\n",
        "    \n",
        "    # Remove any records that have null values for the `salesRank` column. You should apply `dropna()` method to the dataframe `raw_df`\n",
        "    # The value of the parameter `how` should be equal to `\"any\"`\n",
        "    tmp_df = None.None(subset=[\"None\"], how=\"None\")\n",
        "\n",
        "    # Extract the sales rank and category from the `salesRank` column into two new columns: `sales_category` as key and `sales_rank` as value\n",
        "    df_rank = pd.DataFrame([{\"None\": key, \"None\": value} for d in tmp_df[\"salesRank\"].tolist() for key, value in d.items()])\n",
        "\n",
        "    # Concatenate dataframes `tmp_df` and `df_rank`\n",
        "    target_df = pd.concat([None, None], axis=1)\n",
        "\n",
        "    # Select the columns that don't contain arrays and the new columns (this line is complete)\n",
        "    target_df = target_df[cols] \n",
        "    \n",
        "\n",
        "    # Remove any record that has null values for the `asin`, `price` and `sales_rank` column\n",
        "    # You should use `dropna()` method and the value of the parameter `how` should be equal to `\"any\"`\n",
        "    target_df = target_df.None(subset=[\"None\", \"None\", \"None\"], how=\"None\")\n",
        "\n",
        "    # Fill the null values of the rest of the Dataframe with an empty string `\"\"`. Use `fillna()` method to do that\n",
        "    target_df = target_df.None(\"\")\n",
        "    \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return target_df\n",
        "\n",
        "processed_metadata_df = process_metadata(raw_df=metadata_sample_df, \n",
        "                                         cols=['asin', 'description', 'title', 'price', 'brand','sales_category','sales_rank']\n",
        "                                         )\n",
        "processed_metadata_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3.2.2. Once you have your data with some initial preprocessing, you have to join the data according to the ID of each product (`asin`). Do an inner join in this case as you are interested only in reviews for which you have product information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reviews_product_metadata_df = transformed_review_sample_df.merge(processed_metadata_df, left_on='asin', right_on='asin', how='inner')\n",
        "reviews_product_metadata_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3.2.3. Before starting the next processing steps, you can convert the column names to lowercase. Furthermore, some columns that are not necessary for the model can be deleted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reviews_product_metadata_df.columns = [col.lower() for col in reviews_product_metadata_df.columns]\n",
        "reviews_product_metadata_df.drop(columns=['reviewername', 'summary', 'unixreviewtime', 'reviewtime'], inplace=True)\n",
        "reviews_product_metadata_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3.2.4. In the dataset, you can see that there are two possible categorical variables: `brand` and `sales_category`. Let's explore them to understand their structure and decide the type of encoding that you will implement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reviews_product_metadata_df['brand'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reviews_product_metadata_df['sales_category'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The DataFrame `reviews_product_metadata` now contains information about the reviews and the reviewed products. In the next section, you will apply the following feature engineering steps:\n",
        "- clean the texts in the columns `reviewtext`, `descriptions`, `title`, `brand` and `sales_category`;\n",
        "- process the numerical features `price` and `rank_category` by standardizing their values, and the numerical features `helpful_votes` and `not_helpful_votes` by dividing each by the `total_votes`;\n",
        "- process the categorical features `brand` and `sales_category` by encoding each into numerical features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name='3.3'></a>\n",
        "### 3.3 - Process the Textual Features\n",
        "\n",
        "For text data, you have the following columns `reviewtext`, `description` and `title`. Although the `brand` and `sales_category` do not contain actual text and are actually categorical string variables, you can process those strings in a similar way as the text fields. \n",
        "\n",
        "For that, you will use the `re` python library. The processing steps to be performed over these text and string data will be more focused on cleaning:\n",
        "\n",
        "- Lowercasing: Convert all text to lowercase to maintain consistency.\n",
        "- Remove Punctuation: Strip out unnecessary punctuation.\n",
        "- Remove Special Characters: Clean any special characters that are not relevant to the task.\n",
        "- Remove leading, trailing or multiple spaces.\n",
        "\n",
        "Further processing such as tokenization can be performed by some ML models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3.3.1. Complete the `clean_text` function using the instructions in the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "exercise": [
          "ex05"
        ],
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "def clean_text(text: str) -> str:\n",
        "\n",
        "    \"\"\"Function in charge of cleaning text data by converting to lowercase \n",
        "    and removing punctuation and special characters.\n",
        "\n",
        "    Args:\n",
        "        text (str): Raw text string to be cleaned\n",
        "\n",
        "    Returns:\n",
        "        str: Cleaned text string\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ### (2 lines of code)\n",
        "    # Take the `text` string and convert it to lowercase with the `lower` method that Python strings have\n",
        "    text = None.None()\n",
        "\n",
        "    # Pass the `text` to the `re.sub()` method as the third parameter. This removes punctuation and special characters\n",
        "    text = None.None(r'[^a-zA-Z\\s]', '', None)\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return text\n",
        "\n",
        "columns_to_clean = ['reviewtext', 'description', 'title', 'brand', 'sales_category']\n",
        "\n",
        "for column in columns_to_clean:\n",
        "    # Applying cleaning function\n",
        "    reviews_product_metadata_df[column] = reviews_product_metadata_df[column].apply(clean_text) \n",
        "\n",
        "    # Deleting unnecessary spaces\n",
        "    reviews_product_metadata_df[column] = reviews_product_metadata_df[column].str.strip().str.replace('\\s+', ' ', regex=True)\n",
        "\n",
        "reviews_product_metadata_df[columns_to_clean].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3.3.2. As a last step to process your text data, you are asked to create a column that summarizes the product information. This information can be found in the `title` or `description` fields. You are asked to use the `title` as the primary product information field and for rows where the product's title is not available, you can use the `description`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "exercise": [
          "ex06"
        ],
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "### START CODE HERE ### (3 lines of code)\n",
        "\n",
        "# Take `title` column of the dataframe `reviews_product_metadata_df` and use `replace()` method to replace empty strings with NaN\n",
        "reviews_product_metadata_df['title'] = None['None'].None('', np.nan)\n",
        "\n",
        "# Fill NaN values in title with the description\n",
        "# Use the `fillna()` dataframe method to fill the missing values from the `title` with those from `description` column of the dataframe `reviews_product_metadata_df`\n",
        "reviews_product_metadata_df['product_information'] = None['title'].None(None['description'])\n",
        "\n",
        "# Drop `title` and `description` columns of the dataframe `reviews_product_metadata_df`\n",
        "None.None(columns=['None', 'None'], inplace=True)\n",
        "\n",
        "### END CODE HERE ###\n",
        "\n",
        "reviews_product_metadata_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Other types of processing steps for text data may include deleting stop words, lemmatization or tokenization, but the utility of those steps may depend on the type of ML model to use or can directly be done by the model itself. For this lab, you are not required to implement further processing steps for the text data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name='3.4'></a>\n",
        "### 3.4 - Process the Numerical Features\n",
        "\n",
        "3.4.1. You can see that the joined dataset has, apart from the label `overall` (overall ratings), two numerical variables: `price` and `sales_rank`. You have already processed this type of data in the previous lab, so you will implement a similar approach by performing a standardization over these two variables using `scikit-learn`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "exercise": [
          "ex07"
        ],
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "reviews_num_columns = [\"price\", \"sales_rank\"]\n",
        "\n",
        "### START CODE HERE ### (3 lines of code)\n",
        "\n",
        "# Create a `StandardScaler` instance\n",
        "reviews_num_std_scaler = None()\n",
        "\n",
        "# Compute the mean and standard deviation statistics over the `reviews_num_columns` of the dataframe `reviews_product_metadata_df`\n",
        "# Use `fit()` method applied to the `reviews_num_std_scaler`\n",
        "None.None(None[None])\n",
        "\n",
        "# Perform the transformation over the `reviews_num_columns` of the datafram `reviews_product_metadata_df` \n",
        "# with the `transform()` method applied to the `reviews_num_std_scaler`\n",
        "scaled_price_sales_rank = None.None(None[None])\n",
        "\n",
        "### END CODE HERE ###\n",
        "\n",
        "# Convert to pandas DF\n",
        "scaled_price_sales_rank_df = pd.DataFrame(scaled_price_sales_rank, columns=reviews_num_columns, index=reviews_product_metadata_df.index)\n",
        "\n",
        "scaled_price_sales_rank_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3.4.2. Add the `scaled_price_sales_rank_df` to the `reviews_product_metadata_df` DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop the original column values\n",
        "reviews_product_metadata_df.drop(columns=reviews_num_columns, inplace=True)\n",
        "\n",
        "# Add the scaled values\n",
        "reviews_product_metadata_df = pd.concat([reviews_product_metadata_df, scaled_price_sales_rank_df], axis=1)\n",
        "\n",
        "reviews_product_metadata_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3.4.3. As a last step to process your numerical variables, you are going to create a pair of new features based on the `helpful_votes`, `total_votes` and `not_helpful_votes`. The meaning of those three fields is the following: \n",
        "- `helpful_votes` represents the number of users who found the review helpful.\n",
        "- `total_votes` represents the total number of users who voted on the review's helpfulness.\n",
        "- `not_helpful_votes` represents the number of users who didn't find the review helpful.\n",
        "\n",
        "Create two new features based on the ratios between these columns, named `helpful_ratio` and `not_helpful_ratio`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helpful ratio\n",
        "reviews_product_metadata_df['helpful_ratio'] = reviews_product_metadata_df[['helpful_votes', 'total_votes']].apply(lambda x: x['helpful_votes']/x['total_votes'] if x['total_votes'] != 0 else 0, axis=1)\n",
        "\n",
        "# Not helpful ratio\n",
        "reviews_product_metadata_df['not_helpful_ratio'] = reviews_product_metadata_df[['not_helpful_votes', 'total_votes']].apply(lambda x: x['not_helpful_votes']/x['total_votes'] if x['total_votes'] != 0 else 0, axis=1)\n",
        "\n",
        "reviews_product_metadata_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name='3.5'></a>\n",
        "### 3.5 - Process the Categorical Features\n",
        "\n",
        "You can find two categorical variables in the dataset: `brand` and `sales_category`. When you explored the sample data, you saw that the `sales_category` column has 7 nominal categories, while the `brand` column has 86 categories. The column `sales_category` can be encoded using one hot encoding. However, using one hot encoding for the column `brand`, which has a large number of unique categories, can pose challenges for some ML models in terms of memory and computational complexity. In the lecture, you've learned about label encoding which could be used here, but there are also other types of encodings (you can check this [article](https://medium.com/anolytics/all-you-need-to-know-about-encoding-techniques-b3a0af68338b) for a list of these methods), where each type of encoding can have a different effect on the performance of the ML model. In this lab, you will use an approach named frequency encoding or count encoding. This technique encodes the categorical features based on the frequency of each category. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3.5.1. In the following cell, you will compute the frequency of each category of the `brand` column with the `value_counts` method, then, those categories are mapped back to the DataFrame so instead of having the string categories, you will have the frequency of each category."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Frequency encoding\n",
        "frequency_encoding_brand = reviews_product_metadata_df['brand'].value_counts().to_dict()\n",
        "reviews_product_metadata_df['encoded_brand'] = reviews_product_metadata_df['brand'].map(frequency_encoding_brand)\n",
        "\n",
        "# Dropping the brand column\n",
        "reviews_product_metadata_df.drop(columns=[\"brand\"], inplace=True)\n",
        "\n",
        "reviews_product_metadata_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3.5.2. For the `sales_category` column, you can use the `OneHotEncoder` class given that the number of categories is not that large."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "exercise": [
          "ex08"
        ],
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "### START CODE HERE ### (5 lines of code)\n",
        "\n",
        "# Create an instance of the `OneHotEncoder` class. Use the `\"ignore\"` value for the `handle_unknown` parameter\n",
        "sales_category_ohe = None(None=\"None\")\n",
        "\n",
        "# Copy the column `sales_category` of the dataframe `reviews_product_metadata_df` with the method `copy()`\n",
        "# You will need to use double square brackets to output it as a dataframe, not a series\n",
        "sales_category_df = None[[\"None\"]].None()\n",
        "\n",
        "# Convert string categories into lowercase (the code line is complete)\n",
        "sales_category_df = sales_category_df.map(lambda x: x.strip().lower()) \n",
        "\n",
        "\n",
        "# Fit your encoder `sales_category_ohe` to the `sales_category_df` dataframe with the `fit()` method\n",
        "None.None(None)\n",
        "\n",
        "# Apply the transformation using the same encoder over the same column. You will need to use the `transform()` method\n",
        "# Chain `todense()` method to create a dense matrix for the encoded data\n",
        "encoded_sales_category = None.None(None).None()\n",
        "\n",
        "### END CODE HERE ###\n",
        "\n",
        "# Convert the result to DataFrame\n",
        "encoded_sales_category_df = pd.DataFrame(\n",
        "    encoded_sales_category, \n",
        "    columns=sales_category_ohe.get_feature_names_out([\"sales_category\"]),\n",
        "    index=reviews_product_metadata_df.index\n",
        ")\n",
        "\n",
        "encoded_sales_category_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3.5.3. Add the new dataset with the transformed values to the original dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop the original column values\n",
        "reviews_product_metadata_df.drop(columns=[\"sales_category\"], inplace=True)\n",
        "\n",
        "# Add the scaled values\n",
        "reviews_product_metadata_df = pd.concat([reviews_product_metadata_df, encoded_sales_category_df], axis=1)\n",
        "\n",
        "reviews_product_metadata_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name='4'></a>\n",
        "## 4 - Split Data and Create Text Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name='4.1'></a>\n",
        "### 4.1 - Split Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, that you have your dataset processed, you are going to split it into three subdatasets. The first one will contain all the information about the user's review and product features, except the `reviewtext` and `product_information` columns; those two columns will be stored in two different datasets and you will refer to them through the `reviewerid` and `asin` identifiers to join with the main table. You will make this split to process reviews and product information only once for each unique review or product value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creating reviews dataset\n",
        "reviews_text_df = reviews_product_metadata_df[[\"reviewerid\", \"asin\", \"reviewtext\"]].copy()\n",
        "reviews_text_df.drop_duplicates(inplace=True)\n",
        "\n",
        "# Creating products dataset\n",
        "product_information_df = reviews_product_metadata_df[[\"asin\", \"product_information\"]].copy()\n",
        "product_information_df.drop_duplicates(inplace=True)\n",
        "\n",
        "# Dropping unnecessary columns from the original DataFrame\n",
        "reviews_product_metadata_df.drop(columns=[\"reviewtext\", \"product_information\"], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reviews_text_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "product_information_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reviews_product_metadata_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reviews_product_metadata_df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name='4.2'></a>\n",
        "### 4.2 - Create Text Embeddings\n",
        "\n",
        "Now that the data has been separated into three different datasets, let's create embeddings for the text data. You will create embeddings for the `product_information` field in the `product_information_df` DataFrame and the `reviewtext` field in the `reviews_text_df` DataFrame.\n",
        "\n",
        "Just as a refresher, an **embedding** is a numerical representation of text, documents, images, or audio. This representation captures the semantic meaning of the embedded content while representing the data in a more compact form. The ML team has provided you with an API with the Sentence Transformer library, running the [`all-MiniLM-L6-v2`](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) model, which is a multipurpose and lightweight sentence transformer model that can map sentences and paragraphs to a 384-dimensional vector. You've seen in the lecture how you can interact with such a model using the `sentence-transformers` [module](https://sbert.net/index.html). For this lab, the model has been provisioned on an EC2 instance, allowing you to interact with it through API calls. This approach aligns with the current industry standard for accessing LLM services."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4.2.1. The `ENDPOINT_URL` variable that you defined at the beginning of the lab is the endpoint that you will use to interact with the API. Furthermore, you are provided with a function to make API requests for the model to return the embeddings of a provided text. You should already be familiar with requests to REST APIs, so you can see that this particular call is a POST call in which you also send data to the endpoint through the `payload` dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_text_embeddings(endpoint_url: str , text: Union[str, List[str]]): \n",
        "    \n",
        "    payload = {\"text\": text}\n",
        "\n",
        "    headers = {\n",
        "        'accept': 'application/json',\n",
        "        'Content-Type': 'application/json'\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(endpoint_url, headers=headers, data=json.dumps(payload))\n",
        "        response.raise_for_status()\n",
        "  \n",
        "        return response.json()\n",
        "    \n",
        "    except Exception as err:\n",
        "        print(f\"Error: {err}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4.2.2. As an example of the result of the API call, you can execute the following cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Text to send to the API call\n",
        "text = product_information_df.iloc[0]['product_information']\n",
        "\n",
        "# Performing API call and getting the result\n",
        "embedding_response = get_text_embeddings(endpoint_url=ENDPOINT_URL, text=text)\n",
        "embeddings = embedding_response['body']\n",
        "\n",
        "print(f\"Text sent to the API: {text}\")\n",
        "print(f\"Length of the embedding from the response: {len(embeddings)}\")\n",
        "print(f\"A subset of the returned embeddings with the first 10 elements: {embeddings[:10]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can see that the size of the returned embedding is of 384 elements. This size will be the same for all vector embeddings that will be returned. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4.2.3. Let's process the text for the `product_information_df` DataFrame and get the corresponding embeddings. You are going to divide the DataFrame into more manageable chunks (smaller DataFrames); for that, use the `split_dataframe` function provided below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def split_dataframe(df, chunk_size=20):\n",
        "    chunks = list()\n",
        "    num_chunks = (len(df) + chunk_size - 1) // chunk_size \n",
        "    \n",
        "    for i in range(num_chunks):\n",
        "        chunk = df[i*chunk_size:(i+1)*chunk_size]\n",
        "        if not chunk.empty:\n",
        "            chunks.append(chunk)\n",
        "    return chunks\n",
        "\n",
        "# Split the `product_information_df` with the default chunk size of 20 rows per chunk\n",
        "list_df = split_dataframe(df=product_information_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4.2.4. Given that you need to perform several API calls and the dimension of your embeddings is 384 elements, to avoid exceeding the usage of RAM of your development environment, you will perform API calls to get the embeddings of a chunk of data and then insert the corresponding data directly into the database. Let's create the connection to the database through the `psycopg2` package. Then, in a similar way as you did in the Course 1 Week 4 Assignment, you need to enable the `vector` extension in your PostgreSQL database. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "conn = psycopg2.connect( \n",
        "    database=DBNAME, user=DBUSER, \n",
        "    password=DBPASSWORD, host=DBHOST, port=DBPORT\n",
        ") \n",
        "\n",
        "# Set autocommit to true\n",
        "conn.autocommit = True\n",
        "\n",
        "# Create cursor\n",
        "cursor = conn.cursor()\n",
        "cursor.execute('CREATE EXTENSION IF NOT EXISTS vector')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4.2.5. The [`psycopg2`package](https://github.com/pgvector/pgvector-python?tab=readme-ov-file#psycopg-2) provides `pgvector` support for Python, in other words it enables vector similarity search for Postgres. Once you have enabled the `vector` extension, you need to register the vector type with your connection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "register_vector(conn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4.2.6. Then, you can create the `product_embeddings` table. In that table, you will insert the `asin` (product ID), the product information (either the product title or description) and the embedding from the product information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cursor.execute('DROP TABLE IF EXISTS product_embeddings')\n",
        "cursor.execute('CREATE TABLE product_embeddings (asin VARCHAR(15) PRIMARY KEY, product_information TEXT, product_embedding vector(384))')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4.2.7. Complete the code in the next cell to call the API with a chunk of text and then insert it into the vector database. The insertion process code has already been provided to you. It is created as a string with the `INSERT` statement and the values to be inserted are appended through the `value_array` list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "exercise": [
          "ex09"
        ],
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "for id, df_chunk in enumerate(list_df):\n",
        "    \n",
        "    ### START CODE HERE ### (3 lines of code)\n",
        "    \n",
        "    # Convert the `asin` column from the `df_chunk` dataframe into a list with the `to_list()` method\n",
        "    asin_list = None['None'].None()\n",
        "\n",
        "    # Convert the `product_information` column from the `df_chunk` dataframe into a list with the `to_list()` method\n",
        "    text_list = None['None'].None()\n",
        "\n",
        "    # Perform an API call through the `get_text_embeddings` function\n",
        "    # Pass the `ENDPOINT_URL` variable and the chunk of texts stored in the list `text_list` as parameters to that function\n",
        "    embedding_response = None(endpoint_url=None, text=None)\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Inserting the data    \n",
        "    insert_statement = f'INSERT INTO product_embeddings (asin, product_information, product_embedding) VALUES'\n",
        "    \n",
        "    value_array = [] \n",
        "    for asin, text, embedding in zip(asin_list, text_list, embedding_response['body']):\n",
        "        value_array.append(f\"('{asin}', '{text}', '{embedding}')\")\n",
        "                \n",
        "    value_str = \",\".join(value_array)\n",
        "    insert_statement = f\"{insert_statement} {value_str};\"\n",
        "    \n",
        "    cursor.execute(insert_statement)\n",
        "\n",
        "    if id % 5 == 0:\n",
        "        print(f\"Data inserted for batch with id {id}\")\n",
        "        time.sleep(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4.2.8. Check that data has been inserted:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "cursor.execute('SELECT COUNT(*) FROM product_embeddings')\n",
        "cursor.fetchall()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4.2.9. Use the following cell to get the embeddings from the product information of a particular product."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = product_information_df.iloc[11]['product_information']\n",
        "\n",
        "# Performing API call and getting the result\n",
        "embedding_response = get_text_embeddings(endpoint_url=ENDPOINT_URL, text=text)\n",
        "embeddings = embedding_response['body']\n",
        "\n",
        "print(f\"Text: {text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4.2.10. In a vector database, you can perform searches for the most similar elements. In [`pgvector`](https://github.com/pgvector/pgvector) you can use the `<->` operator to get the nearest neighbors by L2 distance. Extract the 5 nearest products according to the L2 distance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "select_statement = f\"SELECT asin, product_information FROM product_embeddings ORDER BY product_embedding <-> '{embeddings}' LIMIT 5\"\n",
        "cursor.execute(select_statement)\n",
        "cursor.fetchall()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4.2.11. You can play with the text of the product information to find some similar products. For example, create a custom text to find similar products. Given that you have used the `classicmodels` dataset previously, let's search for similar products to this or similar descriptions: \"scale car toy\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"scale car toy\"\n",
        "\n",
        "# Performing API call and getting the result\n",
        "embedding_response = get_text_embeddings(endpoint_url=ENDPOINT_URL, text=text)\n",
        "embeddings = embedding_response['body']\n",
        "\n",
        "select_statement = f\"SELECT asin, product_information FROM product_embeddings ORDER BY product_embedding <-> '{embeddings}' LIMIT 5\"\n",
        "cursor.execute(select_statement)\n",
        "cursor.fetchall()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Although no information about cars can be found in the sample dataset, the embeddings were able to relate the word `scale` with `miniature`. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4.2.12. Once the embeddings from the product's information have been stored, compute and store the embeddings for the reviews. Follow the same procedure as the one done for the product information in the next three cells. Create the `review_embeddings` table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cursor.execute('DROP TABLE IF EXISTS review_embeddings')\n",
        "cursor.execute('CREATE TABLE review_embeddings (reviewerid VARCHAR(30), asin VARCHAR(15), reviewtext TEXT, review_embedding vector(384), PRIMARY KEY(reviewerid, asin))')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4.2.13. Use `split_dataframe()` function to split the `reviews_text_df` DataFrame with the default chunk size of 20 rows per chunk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### START CODE HERE ### (1 line of code)\n",
        "list_df = None(df=None, chunk_size=20)\n",
        "### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4.2.14. Complete the code in the next cell to call the API with a chunk of text and then insert it into the vector database. Execute the insertion cell; this **should take less than 10 minutes**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "exercise": [
          "ex10"
        ],
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "# Call the API and insert the data \n",
        "start_time = time.time()\n",
        "\n",
        "for id, df_chunk in enumerate(list_df):\n",
        "    \n",
        "    ### START CODE HERE ### (4 lines of code)\n",
        "\n",
        "    # Convert the `reviewerid`, `asin` and `reviewtext` columns from the `df_chunk` dataframe into a list with the `to_list()` method\n",
        "    reviewer_list = None['None'].None()\n",
        "    asin_list = None['None'].None()\n",
        "    text_list = None['None'].None()\n",
        "\n",
        "    # Perform an API call through the `get_text_embeddings` function\n",
        "    # Pass the `ENDPOINT_URL` variable and the chunk of texts stored in the list `text_list` as parameters to that function\n",
        "    embedding_response = None(endpoint_url=None, text=None)\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Insert the data\n",
        "    insert_statement = f'INSERT INTO review_embeddings (reviewerid, asin, reviewtext, review_embedding) VALUES'\n",
        "    value_array = [] \n",
        "    \n",
        "    for reviewer, asin, text, embedding in zip(reviewer_list, asin_list, text_list, embedding_response['body']):\n",
        "        value_array.append(f\"('{reviewer}', '{asin}', '{text}', '{embedding}')\")\n",
        "        \n",
        "    value_str = \",\".join(value_array)\n",
        "    insert_statement = f\"{insert_statement} {value_str};\"\n",
        "    \n",
        "    cursor.execute(insert_statement)    \n",
        "\n",
        "    if id % 50 == 0:\n",
        "        print(f\"Data inserted for batch with id {id}\")\n",
        "        time.sleep(10)\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"Total time spent {end_time - start_time} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4.2.15. Check that the data has been properly inserted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cursor.execute('SELECT count(*) FROM review_embeddings')\n",
        "cursor.fetchall()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4.2.16. Finally, you can take a text and search for the more similar reviews using the L2 distance. Change the text for a custom message to experiment with the results! As an example, explore the results from this message: \"I didn't like this toy, it was broken!\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text=reviews_text_df.iloc[0]['reviewtext']\n",
        "embedding_response = get_text_embeddings(endpoint_url=ENDPOINT_URL, text=text)\n",
        "embeddings = embedding_response['body']\n",
        "print(f\"Text: {text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "select_statement = f\"SELECT reviewerid, asin, reviewtext FROM review_embeddings ORDER BY review_embedding <-> '{embeddings}' LIMIT 5\"\n",
        "cursor.execute(select_statement)\n",
        "cursor.fetchall()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4.2.17. The last step consists of inserting the data from the `reviews_product_metadata_df` DataFrame, which contains the other transformed variables that can be used to train another ML model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ddl_statement = \"\"\"CREATE TABLE review_product_transformed (\n",
        "    reviewerid VARCHAR(25) NOT NULL,\n",
        "    asin VARCHAR(15) NOT NULL,\n",
        "    overall FLOAT,\n",
        "    year INTEGER,\n",
        "    month INTEGER,\n",
        "    helpful_votes INTEGER,\n",
        "    total_votes INTEGER,\n",
        "    not_helpful_votes INTEGER,\n",
        "    price FLOAT,\n",
        "    sales_rank FLOAT,\n",
        "    helpful_ratio FLOAT,\n",
        "    not_helpful_ratio FLOAT,\n",
        "    encoded_brand INTEGER,\n",
        "    sales_category_arts_crafts_sewing FLOAT,\n",
        "    sales_category_electronics FLOAT,\n",
        "    sales_category_home_amp_kitchen FLOAT,\n",
        "    sales_category_industrial_scientific FLOAT,\n",
        "    sales_category_software FLOAT,\n",
        "    sales_category_sports_amp_outdoors FLOAT,\n",
        "    sales_category_toys_games FLOAT,\n",
        "    PRIMARY KEY (reviewerid, asin)\n",
        ");\n",
        "\"\"\"\n",
        "\n",
        "cursor.execute('DROP TABLE IF EXISTS review_product_transformed')\n",
        "cursor.execute(ddl_statement)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "insert_query = \"\"\"\n",
        "    INSERT INTO review_product_transformed (\n",
        "        reviewerid, asin, overall, year, month, helpful_votes, total_votes, \n",
        "        not_helpful_votes, price, sales_rank, helpful_ratio, not_helpful_ratio, \n",
        "        encoded_brand, sales_category_arts_crafts_sewing, sales_category_electronics, \n",
        "        sales_category_home_amp_kitchen, sales_category_industrial_scientific, \n",
        "        sales_category_software, sales_category_sports_amp_outdoors, sales_category_toys_games\n",
        "    ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
        "\"\"\"\n",
        "\n",
        "# Iterate over the DataFrame rows and insert each row into the database\n",
        "for i, row in reviews_product_metadata_df.iterrows():\n",
        "    cursor.execute(insert_query, tuple(row))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cursor.execute('SELECT COUNT(*) FROM review_product_transformed')\n",
        "cursor.fetchall()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4.2.18. Finally, you are required to close the connection to the vector database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cursor.close()\n",
        "conn.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this lab, you successfully transformed a dataset provided in JSON format into a structured format suitable for machine learning (ML) applications. This process involved three critical steps of feature engineering: data extraction, feature creation, and data storage. In addition, you processed text data and interacted with an NLP model to generate text embeddings. You integrated your process with a vector database in PostgreSQL which facilitates the efficient storage and retrieval of high-dimensional vector data. With the embeddings stored in your vector database, you are also able to find similar items or reviews through the usage of an L2 Euclidean distance to measure similarity between vectors. This approach allowed you to retrieve items or reviews with embeddings that were closest to the query embedding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name='5'></a>\n",
        "## 5 - Upload Files for Grading\n",
        "\n",
        "Upload the notebook into S3 bucket for grading purposes.\n",
        "\n",
        "*Note*: you may need to click **Save** button before the upload."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Retrieve the AWS account ID\n",
        "result = subprocess.run(['aws', 'sts', 'get-caller-identity', '--query', 'Account', '--output', 'text'], capture_output=True, text=True)\n",
        "AWS_ACCOUNT_ID = result.stdout.strip()\n",
        "\n",
        "SUBMISSION_BUCKET = f\"{LAB_PREFIX}-{AWS_ACCOUNT_ID}-us-east-1-submission\"\n",
        "\n",
        "!aws s3 cp ./C4_W2_Assignment.ipynb s3://$SUBMISSION_BUCKET/C4_W2_Assignment_Learner.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
