In my discussions
with data teams from many big tech companies, I've noticed that a lot of a data engineer's work is batch processing to serve
data for analytics, and in some cases, for training machine
learning models. With batch transformations, you manipulate discrete
chunks of data on a fixed schedule
like daily, hourly, every 15 minutes or less to
support ongoing reporting, analytics, and machine
learning use cases. Let's go through some common batch transformation patterns. Suppose you created a
model for your data. It could be based on a
Kimball Star schema, Data Vault, or some other
data modeling approach. Now, you're ingesting
data into your system, and you need to apply
transformations to restructure the source data
into the expected form. You have a few options here. You could follow a
traditional ETL approach and rely on an external
transformation tool to extract and transform the data
based on the data model you created before loading
the transformed data into a target system, such as the data warehouse. Or you can follow an ELT
approach where you extract raw data from a sore system and import it directly
into a data warehouse. Then you clean and transform the data in the
warehouse itself, relying on the
storage and computing capabilities of the warehouse. Or you can take a hybrid
approach called EtLT, where the t refers to the simple transformations
you apply to clean the data, like duplicating the data before you load the clean data
into the data warehouse. Then the T refers to the
transformations you apply inside the data warehouse to restructure the data based on
the model that you defined. In fact, you already
experimented with both the ETL and ELT
approaches in previous labs. Back in Course 1, you
implemented an ETL pipeline with AWS Glue ETL as the external transformation
tool to transform the data before
loading it into S3. AWS Glue ETL is based on a distributed processing
framework called Spark. You can use it to perform more complex transformations
on larger datasets. We'll cover Spark in more
detail later this week. Then in the first
week of this course, you implemented an ELT
pipeline using DBT to transform the data within
a database itself. Note that DBT is not an
execution tool like Spark, meaning it doesn't come
with compute resources, but it is instead
a SQL tool that you can use to facilitate
the transformation task within a database or data
warehouse by relying on the computing resources
of the storage system. In addition to transforming your data into a target schema, you might need to
apply transformations to clean and
normalize your data. For example, the source data you extracted might
have missing values, duplicate entries, outliers,
or other inconsistencies. This process of taking
messy malformed data and turning it into clean data
is called data wrangling. You could write your
own code to perform data wrangling like
you did last week to turn raw data into a useful form for training a
machine learning algorithm, but I highly recommend to use a data wrangling tool to avoid undifferentiated
heavy lifting. There are many third-party data wrangling tools
available to you, and many major cloud providers typically offer their own
version of these tools. For example, AWS offers AWS Glue DataBrew as a visual data preparation
service for cleaning, standardizing, and
transforming data. After you store your
transformed data in the data pipeline, you might need to periodically
or continuously update the data to make
sure that it's in sync with this data
in the source system. You can apply a simple approach known as truncate and reload, where you delete all
records from your table and then reload the data
from the data source, rewriting any
transformations needed to get the data into
your target system. This approach works well when you have a small dataset and only need to update the data in your target system
once in a while. However, if your
dataset is large, this approach can become
very resource-intensive. In this case, you
might want to adopt a CDC or change data
capture approach, where you first identify the changes made in
the source system and then update the tables in your target system based
only on those changes. For example, you can check the Last Updated column in a
relational source database, or you can check the
databases transactional logs. Each row in the log can
be labeled with an I, if the row is inserted, a U, if the row is updated, or a D, if the row is deleted. When handling updated rows, you can apply an
insert-only pattern or upsert/merge pattern to
update your target system. With an insert-only pattern, you insert new records without changing or deleting
old records, and you add additional
information, the new record to distinguish
it from the old one. With an upsert/merge pattern, you take a set of source records and look for matches against your target table by using a primary key or another
logical condition. When a match occurs, you update the target record by replacing
it with a new record. When no match exists, you insert the new record. When handling deleted rows, you can adopt a hard or
soft delete pattern. With a hard delete,
you permanently remove a record from
your target system, whereas with a soft delete, you mark the record as deleted. You might use hard deletes to remove data for
performance reasons, say a table is too big, or if there's a legal or
compliance reason to do so. You can use soft deletes when you don't want to
permanently delete a record, but you also want to filter
it out-of-query results. You can also delete a record in an insert-only manner
when you insert a new record with a deleted flag without modifying the previous
version of the record. Single row inserts are commonly performed on
row-oriented databases. However, a problem I see a lot is that
some data engineers try to perform
single row inserts into an OLAP column
oriented database. This is an anti-pattern
that could put massive load on the OLAP system. It also causes the data to be written in many separate files, which is extremely inefficient
for subsequent reads. Instead, I recommend
loading data in a periodic micro-batch
or batch fashion. When you insert data in bulk, the data can be organized
more efficiently into row groups and
better compressed. If the OLAP system
is distributed, you can leverage the distributed parallel processing capability instead of loading
records one by one. Those are some of the common batch
transformation patterns. Like I mentioned earlier,
if you're only working with simple transformations
and small datasets, you might be able to get
away with performing the transformations
on a single machine. However, as the transformations you need become more complex, and the datasets become larger, you'll need to consider using distributed processing
frameworks to meet the scalability and
performance requirements. While Cloud data warehouses leverage distributed
processing power, you might also need to
apply transformations outside the data warehouse
or inside a data lake. Join me in the next
video to begin our discussion on distributed
processing frameworks.