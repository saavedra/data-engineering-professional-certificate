So far in this course,
you've modeled data for analytical use cases by
defining a normalized schema, a star schema, and a
one big table schema. You've also modeled data for machine learning
use cases and done some simple transformations on small datasets using
Pandas data frames. But when you're working
with large scale data, that requires complex
transformations, Pandas data frames
alone might not offer the scalability and performance you need and so this week, we'll get into the details of
transformations and look at the technical considerations for various data
processing frameworks that you can use to
transform your data. As I already mentioned, the
transformation stage of the data engineering
life cycle is when you manipulate and enhanced data
for downstream stakeholders, and is where you,
the data engineer, can really add value
to the organization. For example, you can
clean and combine data from multiple
sources and then store it in a data lake or
data warehouse to create a central source of
truth for your organization. Inside your data warehouse, you can leverage its massively parallel processing ability to transform the data into a star schema
or a data vault so that users can more
easily query the data. Inside your data lake, you can apply a series of transformations by
moving your data from the raw to the cleaned
or transformed zone. Finally, to the
enriched zone so that it's ready to be
consumed by data users. To apply these transformations, you need to consider things
like the size of the data, specification of hardware
that's available to you, and the performance
requirements. These considerations
will help you decide whether you can just process the data on a single machine, or use a distributed
processing tool like Spark, and whether you should write
your transformation logic in SQL or another
language like Python. You can also apply transformations
to streaming data, allowing your stakeholders to perform real time analytics. In this case, you'll need to consider the latency
requirements of the system and ensure that your transformations
don't incur any delays. This week, we'll start
by going through some of the batch transformation
use cases you might encounter
as a data engineer. Then we'll cover two distributed
processing frameworks. Hadoop MapReduce, which uses disks for storing
and processing data, and Spark, which is an in
memory processing framework. Many data engineers
consider Hadoop to be a legacy technology
due to its complexity, the high cost of scaling, and significant
maintenance requirements. But I also think that it's important for you to understand
the MapReduce paradigm, since it influences many of
today's distributed systems. We'll compare SQL based
transformations with those implemented in other
languages such as Python. In the lab, you'll get
a chance to perform the same transformations
you did with DBT previously in this course, but this time, you'll implement these transformations
outside the data warehouse. This week, you'll also
hear from Navnit Shukla, a senior solutions
Architect at AWS, and an expert on the
AWS Glue Service, which is a tool built on Spark. He'll show you how
you can generate the Glue jobs for processing
your data using Glue Studio. Then in the second lesson, we'll take a look at
streaming transformations. You'll implement a
change data capture or CDC pipeline using Kafka and Flink to capture changes in our data source and update the data in your
system accordingly. It's going to be a
jam packed week. Join me in the next
video to get started.