To host any software application, you need
a server, which is essentially a computer. Or a collection of computers that powers
your application by providing CPU, memory or ram as well as disk storage and
maybe a GPU and networking. Servers provide computing resources over a
network, most often through the Internet. When it comes to cloud tools, depending on
the service you're considering, you may be required to set up and manage the compute
resources required to run the application. And in other cases,
you may be able to choose between one or more of the following
three computing options. Server container or
serverless in this video, I'll go over the differences and
trade offs between these three options. If you choose to go with
the server version of a service, you will be responsible for
setting up and managing the server, such as an Amazon EC2 instance
that the service will run on. Including updating the operating system,
installing or updating packages, software packaging, networking,
scaling and security. In contrast to a server, a container
is a more modular unit that wraps up your code and all its dependencies
into a package that can run on a server. So whereas a traditional virtual
machine wraps entire operating system. A container is more lightweight as
it just packages up in an isolated user space such as a file system and
a few processes. So for a containerized solution,
you would still be responsible for setting up the essential elements of
the application code and dependencies. But the underlying operating system,
networking and all the rest would be provided apart from
server and container options for services. It's becoming increasingly common
in the world of cloud data tools to encounter the terminal serverless
to describe a particular service. If you're familiar with the way computers
work, a word like serverless might sound a little strange, like how do you run
software without a server, right? Well, it turns out that the term
serverless doesn't actually mean there is no server. It just means that setting up and
maintaining the server is not your responsibility for
that particular service. You can interact with the application
without managing the servers behind the scenes of or worrying about package
installations and dependencies. So the server is essentially hidden
from you, like I've drawn here. Typically, serverless technologies
run on containers, so these services can automatically scale. They have availability and
fault tolerance built in, and they offer pay as you go billing. But in the case of serverless services, the containers they run on
were also abstracted away. In this way, serverless technologies can
allow you to spend less time worrying about your compute infrastructure and more
time focusing on developing data products. In the previous week's lab, you worked
with a number of serverless services such as Amazon, Athena and AWS Glue. The serverless trend kicked off in full
force with the launch of AWS Lambda in 2014, which is a service that allows you
to run code in response to an event. With the promise of executing small chunks
of code on an as needed basis without having to manage a server. Serverless options have
exploded in popularity and diversity, and now go far beyond just
running code snippets on demand. The main reasons for
this popularity are cost and convenience. Instead of paying the cost of a server,
you can just pay a little bit each time your code is run or
when you use a particular service. So when does it make sense
to use serverless services? As with many other cloud services,
it depends. As a data engineer, you need to understand
the details of cloud pricing to be able to predict the cost of your
serverless deployments and decide if it's more cost
effective than the server option. For example,
looking at the pricing for AWS Lambda, you'll find that using the service
in an environment with high rates of events can be catastrophically expensive. As in other areas of your data pipelines,
it's critical to model and monitor the services you use,
serverless or otherwise. You may need to monitor directly to
determine actual event rates, duration and cost per event in a real
world environment. So you can model the overall cost
of a serverless service versus some alternative. Apart from that, cloud serverless
platforms have limits on execution, frequency, concurrency and duration. If your application can't function
neatly within these limits, it's time to consider a container oriented approach so
you can think about it like this. Serverless works best for simple,
discrete tasks and workloads. It's not going to work as well
if you have many moving parts or require a lot of compute or
memory horsepower. In that case,
consider using containers and a container workflow orchestration
framework like kubernetes. For most modern data engineering
applications in the cloud, you can get the job done with
serverless or containerized tools. So I would recommend looking at using
serverless first, and then containers and orchestration if possible. And once you've outgrown
these serverless options. Join me in the next video to wrap
up this lesson with a look at how the undercurrents of the data
engineering lifecycle come into play when choosing tools and technologies for
your data architecture.