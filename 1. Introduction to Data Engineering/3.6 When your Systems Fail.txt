Apart from building data systems that serve the needs
of stakeholders, break down salos across teams, and evolve with the changing
needs of your organization. If that's not already enough, you also need to anticipate what happens when
things go wrong. Believe me, things
will go wrong. In the next group of principles, I have planned for failure, architect for scalability, prioritized security,
and embrace FinOps. When it comes to failure
modes in your systems, it's best to take a practical
and quantitative approach, just like you would with any other aspect of the
performance for your system. Now we get to define
more explicitly what is meant by terms that
describe your system metrics, like availability,
reliability and durability of your data systems. Availability often called
uptime is the percentage of time a service or component is expected to
be in an operable state. If you take a look at the
different storage classes and Amazon S3 object
storage, for example, you'll see that they range
in availability from 99.5% up to 99.99% over
the course of the year. Now, 99.5% and 99.99% might sound like
high availability and even very similar numbers, but keep in mind that a 99.5%
annual availability means that you can expect your
storage system to be unavailable for around 44
total hours each year. While 99.99% availability means you can expect just about one
hour of downtime per year. Unfortunately, 100% availability is never possible to guarantee, since failure
scenarios can include unexpected power outages or impairment of network devices. But depending on the
needs of your system, you can choose a storage class with the availability
that you require. Reliability is similar
to availability, but is instead the
probability of a particular service or
a component to perform its intended function within
a given time interval, within well defined
performance standards. Durability refers
to the ability of a storage system to withstand data loss due to
hardware failure, software errors, or
natural disasters. In the Cloud, durability
is crucial as businesses rely on Cloud services to store and access
to critical data. Example, Amazon S3 boasts very high durability
at 99.99999999. That's probably too many
9s to say out loud, but that's a total of 11
9s of data durability. Meaning the loss of objects in Amazon S3 is extremely rare. Related to the concepts of availability, reliability
and durability, are the recovery time
objective or RTO, for sure and the recovery point objective or RPO, for sure. The RTO is your maximum
acceptable time for a service or system outage. To establish an RTO
for your application, you want to consider
the impact to internal and external customers if this application
is unavailable. Then you can, for example, decide on the S3 storage
class to meet the subjective. RPO, on the other hand, is a definition of the
acceptable state after recovery. For example, when talking
about a data storage system, the RPO might refer to the maximum acceptable data loss your system can tolerate
due to an outrage. Being deliberate about
your RTO and RPO for these systems you
build will help you choose components with
the right availability, reliability, and durability
specs to meet your needs. That's one aspect of what it
means to plan for failure. Another way your system can fail is through
security breaches. That's why the principle of
planning for failure and the principle of prioritizing
security go hand in hand. We already talked a little
bit about cultivating a culture of security and the principle of
least privilege. Here, I'd also like to introduce what's called zero
trust security. To understand what zero
trust is all about, it's useful to take
a look at what you might call a more traditional
approach to security, which is known as hardened
perimeter security. Taking a hardened
perimeter approach is equivalent to
building a big wall, like this around
your systems where everyone and everything
outside the wall is untrusted, while everything and every
one inside the wall is trusted in the sense of having access to sensitive
data and systems. The problem with a hardened
perimeter approach is that would be attackers only
need to get past the wall, to gain unfettered access to all of your data
and your systems. In the Cloud era, the idea of building a hardened perimeter has the
additional problem of there really being no
physical perimeter with data and systems connecting
across the Internet. Zero trust by contrast, means that every action
requires authentication, and you build your
system such that no people or applications, internal or external
are trusted by default. Instead, access is
granted only as needed. Your system can also fail
when you do things like incur large unforeseen costs or miss opportunities
for revenue. For example, when it comes
to unforeseen costs, I'm talking about things
like accidentally running expensive
Cloud services such that your entire annual budget is consumed in a month or less. Believe it or not, I've
seen this happen a lot? Or conversely, a
missed opportunity might be getting a sudden
spike in demand for your products and having your whole system
crash because you weren't well prepared
to scale up quickly. In this way, the principles of embracing FinOps and
architecting for scalability, are connecting to the principle
of planning for failure. As a data engineer, you
need to think about the cost structures
of Cloud systems. For example, when running
a distributed cluster, what is the appropriate
mix of AWS on demand, EC2 instances versus
spot instances? By the way, spot instances or the unused EC2
instances that are available at a sharp
discount on AWS. What is the most appropriate
approach for running a sizable daily job in terms of cost effectiveness
and performance? In the Cloud era,
most data systems are pay as you go and
readily scalable. Systems can run at a
cost per query model, cost per processing
capacity model, or another variant of
a pay as you go model. It's now possible
to scale up for high performance and then
scale down to save money. However, the pay as you go approach makes spending
far more dynamic. The new challenge
for data engineers is to manage budgets, priorities, and efficiency when building and maintaining
their systems. The main takeaways here are
that if you plan for failure, or architect for scalability, prioritized security,
and embrace FinOps, you'll be better positioned to serve the needs of
your organization, not only when the systems you're building are
functioning as expected, but also when failures happen. Next up, we'll dig
into the details of some specific
architecture approaches for different types
of data systems. Join me in the
next video to take a closer look at
batch architectures.