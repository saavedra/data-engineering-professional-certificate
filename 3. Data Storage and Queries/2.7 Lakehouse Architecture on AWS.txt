You've learned a lot
already from Joe about the differences between data
lakes, data warehouses, and data lake houses, and you've completed
a lab using AWS Glue and Amazon S3 to set
up a simple data lake. It's common for
organizations to start with a simple data lake
like this and then evolve over time to use a more mature solution
like a data lake house. Now it's time to
discuss how you can architect a data lake
house using AWS services, including AWS lake formation, and Amazon Redshift Spectrum. At its core, AWS lake
formation is designed to simplify the process of building
and managing data lakes. Traditionally, setting
up a data lake or lake house involves a
lot of manual steps, defining storage, setting
up access controls, cataloging data, and managing
permissions to data assets. This can be complex
and time consuming, and lake formation automate
some of these tasks, making it simpler for
you to get started. Here's how it works, you start by identifying your
existing data sources, like Amazon S3 or relational
and NoSQL databases. Then you can use
lake formation to move that data into
your data lake. After that, you
use lake formation to crawl through the
data, catalog it, and get it ready for analytics. Finally, you can give
your users secure self service access to this data with their
preferred analytics tools. It's a streamlined way
to make sure everyone in your organization can easily find and use the data they need. You may recognize
some of these tasks as things you've done
before with AWS Glue and that is because lake
formation is actually built on top of
AWS Glue and IAM, so it's using
features of Glue you are already familiar
with like glue jobs, workflows, and crawlers
to perform these tasks. When you're using
lake formation, you can create things
like workflows, but also manage these features directly in the Glue console. With a typical data
lake or lake house, there are many AWS services
interacting with each other and with end users
accessing different datasets. Along with that comes
a fair amount of overhead for managing
permissions and so how part of lake formation
helps automate the creation of data lake is managing complex
fine grained permissions. Lake formation also provides fine grained access control
on the data stored in S3, and the metadata in
the data catalog. You can centrally
manage permissions and IAM policies to
streamline the process of governing and sharing
your data internally and externally for both analytics and machine learning
applications. Now that you've got a high level understanding
of lake formation, let's review a diagram
of an example of data lake house architected
with AWS services. We'll dive deeper into various aspects of this
architecture as we go along. First, you have
the data sources, which from a data
engineer's perspective, are often out of your control. These are things like databases, file shares, SAS
applications, and more. Then you have the
ingestion mechanisms, which would ingest data
into the data lake house. As you have learned
in previous courses, there are various services you can use for data ingestion, including Amazon
Kinesis Data Streams, Amazon Data Firehose, AWS Data Sync, AWS database migration
service, and Amazon AppFlow. Also, AWS lake
formation can manage some data ingestion
tasks through AWS Glue. All that ingested data needs
to be stored somewhere, so n ext to the ingestion layer, there is the storage
layer that uses Amazon Redshift and Amazon S3. Above that, sits the
processing layer, where you would read data from the Lake house storage layer and transform it for
downstream consumers. You would use services like
Amazon EMR or AWS Glue, Amazon manage service
for Apache Flink, or SQL data processing
on Amazon Redshift. Then there's the catalog layer that uses lake
formation to provide a central catalog
to store and manage metadata for all datasets
hosted in the storage layer. In this layer, you can also
use lake formation to manage permissions and provide fine
grained access control. Then finally, the
rightmost layer here for the lake house architecture
is the consumption layer, which provides AWS services you may use to consume the data, including but not limited to Amazon Sage maker for
machine learning use cases, Amazon QuickSite for business intelligence
and data visualizations, and Amazon Athena and Amazon Redshift Spectrum for querying data in the lake house. We will spend more
time exploring Amazon Redshift Spectrum
in the next video. Since you have already
learned a lot about data sources and
ingestion use cases, I won't focus on
those more right now. But I would like for
you to join me in the next video for a closer
look at the storage, processing, catalog, and
consumption layers of these data lake house
architecture. I'll see you there.