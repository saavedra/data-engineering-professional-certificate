As a data engineer, you're faced with
a large number of cloud storage options. These storage systems exist at a level of
abstraction above the raw ingredients that you saw previously. In this video, we'll look at the three
common types of cloud storage systems. These are block storage,
file storage, and object storage. You'll need to consider the performance
and scalability trade offs between these options when choosing the best system for
your use case. Let's start with the file storage systems,
which organize files into directory trees. Similar to how the folders might
be organized on your laptop. Each folder contains metadata for
its files and subfolders, detailing the names, owners last
modification dates, access permissions. And location pointers to the actual
files and subfolders themselves. So to locate a file on disk, you give
the operating system a path to follow, like this one. Following the hierarchical
structure from left to right, your operating system starts at the root
directory indicated by the forward slash. Then it finds the users directory,
then the matthewhousley subdirectory, and finally it locates
the file named output.txt. In your work as a data engineer, you use file storage when you need to
provide centralized access to files that need to be easily shared and managed
by multiple users or host machines. You can use a managed cloud
file storage service like Amazon Elastic File System or EFS. This service provides you,
your applications, and your stakeholders access
to shared files over network. Without the hassle of managing networking,
scaling disk clusters, or configuration. File storage is often built
on top of block storage, where the complexity of the underlying
storage mechanisms is abstracted from you. Despite being a more accessible and
understandable storage format file, storage systems don't
have the highest read and write performance because they need
to keep track of the file hierarchy. Block storage, on the other hand, provides
the performance of flexibility needed for high-speed transactional data access. Block storage divides files into small, fixed sized blocks of data that you
can store on magnetic disks or SSDs. This allows you to precisely allocate
storage space for any given piece of data. Each block has a unique identifier like
the address for that block which helps you efficiently retrieve and
modify data in individual blocks, providing higher performance and
lower latency than file storage. You often design block storage systems
based on a distributed architecture. Spreading blocks of data
across multiple storage disks, which leads to higher scalability and
stronger data durability. This makes block storage the backbone
of most modern storage solutions. Including your local file systems,
transactional databases, and virtual machine storage. When you store a file in block storage, the storage application writes
the data into multiple blocks. And records the blocks identifier
into a data lookup table. When you request a specific file,
the application retrieves the data from the relevant blocks and merges them into
the original sequence for you to read. This is all abstracted from you. So you can locate any block
by its unique identifier and update the block without having
to replace the entire file. This makes block storage ideal for
use cases where your data is accessed and modified often. For example, transactional database
systems generally access disks at a block level for
high random access performance. This enables OLTP systems to
perform small, infrequent read and write operations with low latency. Block storage is also used to
provide persistent storage for virtual machines like EC2 instances. So in the labs when you created all those
cloud nine environments that ran on EC2 instances. You automatically attach a root storage
device that's backed by a block storage volume to each of your instances. You can install the operating system,
file system, and other computing resources
in the block storage volume. With EC2, the default storage is
Amazon Elastic Block Store or EBS. And you can choose from various EBS
volume types depending on your use case. For example, some volumes are built on
high performance SSDs and are great for latency-sensitive workloads. While others use cost efficient
magnetic disks to store data that's infrequently-accessed. Since block storage volumes are typically
attached to compute instances, scalability is limited by how much you
can scale your compute resources, and so block storage typically caps
out at a few terabytes. Object storage, on the other hand, decouples the data storage
layer from the compute layer. So it can scale to petabytes of storage or
more in a cloud environment where your storage capacity is
limited only by budget. With object storage, you'll likely run out of money before
you run out of object storage space. And so cloud object storage allows you to
process data with ephemeral clusters and scale these clusters up and
down on demand. These ephemeral clusters
exist behind the scenes, and you don't need to worry about them. This is a big factor in making big data
available to smaller organizations. That can't afford to own hardware for data
jobs that they'll only occasionally run. Let's review what you learned about
object storage in the context of source systems back in course 2. Unlike a traditional hierarchical file
storage system, you store files as immutable data objects in a flat
structure in an object storage system. You organize objects into top-level
logical containers like an S3 bucket. And each object is assigned
a unique identifier or key that you can use to find
the object within its container. So an object identifier in S3
might look something like this. The first part refers to the bucket name,
which must be unique across all of AWS and data-example.json is the key
pointing to the particular object. Once you write the data initially,
the object becomes immutable. Even if you want to change
one character of a 1gb file, you'll have to rewrite the entire object
instead of just making a small change like you would in block storage. This might seem like a constraint, but it actually removes the overhead of
supporting chain synchronization. So you can distribute the objects across
many storage nodes that each contain their own disks eliminating the need to
propagate data changes across all these nodes. This allows object stores
to scale horizontally and support extremely performant parallel
reads and writes across many disks. Each node holds shards of objects, which
are replicated across multiple nodes for durability. This high scalability and
durability makes object storage ideal for the storage layer of cloud data
warehouses and data lakes. It allows these storage abstractions to
accommodate massive volumes of data in a cost efficient way. But since objects are immutable, object storage is not good at supporting
transactional workloads where many small update operations
need to happen with low latency. Instead, object storage are great for
storing data needed in massive OLAP systems that focus on read
heavy analytical workloads, rather than write heavy
transactional operations. In modern data engineering applications, object storage also plays a crucial
role in machine learning pipelines. That require large amounts of unstructured
training data such as raw text, images, videos, and audio. And so file, block and
object storage all have wide use cases. If your focus is on
simple data sharing and ease of management with low performance
and scalability requirements then file storage systems might be
the most straightforward solution. To support transactional workloads
that require frequent read and write operations with low latency,
you should choose block storage. And if you need to perform analytical
queries on massive datasets, you can choose object storage for
its high scalability and parallel data processing capabilities. Cloud providers typically offer different
storage tiers within each of these three common storage options. Join me in the next video to take a look
at how to decide on an appropriate storage tier by considering the hot, warm, and cold classification method used to
categorize data based on access frequency.