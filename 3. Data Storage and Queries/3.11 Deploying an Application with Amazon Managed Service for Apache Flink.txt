You just heard from Joe about how you
can use tools like Apache Flink to query streaming data, and
in the upcoming lab you will have an opportunity to get hands
on with Apache Flink. When you're looking to run
Apache Flink with AWS, you have multiple options to choose from. You can run Apache Flink on
Amazon EMR as a yarn application. Or you can self host Apache Flink in
a containerized environment using Amazon Elastic Kubernetes service or
Amazon Elastic container service. These are what you might call the do
it yourself options, but you can also choose to use a managed service like
Amazon managed service for Apache Flink. And that's what I'm going
to talk about here. Amazon managed service for
Apache Flink runs Apache Flink on AWS. It provides the underlying infrastructure
for your Apache Flink applications and creates a hosted,
serverless environment for them to run in. It handles a lot of the heavy lifting for
you, including provisioning, compute,
setting up and managing AZ failover for resilience, automated scaling,
and application backups. To give you a sense of how this works, I'll run through a demo on how to set up
Amazon managed service for Apache Flink, and we'll cover some of the core
concepts along the way. Here I am in the AWS management console
and I will type Flink in the search bar and then select Amazon
manage service for Apache Flink. Manage Apache Flink creates
the hosted environment for you to run your Apache Flink applications. But you would write your applications
locally, just like you would with any other programming project
using the Apache Flink framework. Under get started, I can choose to
either create an application or I can choose studio notebooks. Creating an application is what you
would choose if you wanted to host your Apache Flink application and
run it in a production environment. This involves defining
the necessary resources, configuring the application settings,
and deploying your Flink job. AWS then handles the underlying
infrastructure, scaling and operational aspects, allowing you
to focus on your application logic. On the other hand, studio notebooks
are ideal for development and interactive data exploration. They provide a browser based interface
powered by Apache Zeppelin that is integrated with Apache Flink. And it allows you to run stream processing
applications using standard SQL, Python, and Scala. Using this interactive approach,
you can experiment with your flink code, test different scenarios, and visualize
results quickly and interactively. This is particularly useful during
the development phase or for an ad hoc data analysis task. For the first part of this demo,
I'll create a streaming application. Then later I'll create a studio notebook. On the next page, you need to select
whether you want to start from scratch or from a blueprint. I am going to select a blueprint which
will create all the resources you need to get started using AWS cloudformation. It will set up an Amazon Kinesis data
stream as the source we want to analyze. Then it will set up the resources needed
to send demo data into the stream. >> And deploy an application that will
read from the stream, process the data, then send the process data to
Amazon s three to be stored. I will select deploy blueprint and cloudformation will now deploy
the necessary resources. This will take a few minutes to complete,
so we will come back when it's done. All right, the cloudformation
template is now done deploying and our demo application is now running. This demo application sends sample
data for stock ticker prices through the kinesis data stream, simulating
stock market data coming in real time. And then deploys a flink application that
does a simple data transformation and then writes the data to S3. This blueprint gives you a link to
the GitHub repo where you can explore the Apache flink application it deployed
and this one was written in Java. A lot of this code is boilerplate and
used to get things set up. So I want to direct your attention to
the part of the program that actually does the transformation. In this run app with
Kinesis source method, there is logic that sets up the source
stream to be Amazon Kinesis data streams. Then it runs the transformation logic. This code reads stock data from an Amazon
Kinesis stream into a flink data stream, filters the data to keep only stocks
with a price of $1 or higher, and then writes the filtered
data to an s three file. Now that you understand what the
application is, let's head back to the AWS console where we can then select
the open Apache Flink dashboard button. This opens the Apache Flink dashboard
where there is one job running. If we select running
jobs in the navigation, there is the blueprint job called Kinesis
data streams to S3 Flink streaming app. Clicking that to view more details. We can then see a graph of
the operators we defined for this job. We are reading the stock data from
the stream using a flat map to filter out stocks with the price lower than $1,
and then writing the filter
data to an S3 file. The graph visually represents the flow
of data through these operators, making it easier to understand
the processing steps and how the data is being transformed and
stored. If I select this operator here, you can see that 10,000
records have been processed. All right, so that was an example
of running an Apache Flink application with a data processing and
analysis job, pulling data from
Amazon Kinesis data streams. As I mentioned earlier, another way you
can use Flink on AWS is through studio notebooks, which can be great for
data exploration or ad hoc analysis. So that's what I'd like to show you next. Join me in the next video to deploy
a studio notebook using Amazon managed service for Apache Blink.