Using joins is one of the most common ways
to combine datasets, allowing you to
transform data and create new datasets within
your data pipeline. Additionally, your
end users might also use joins to combine the
data you serve to them. However, joins are one of the most time-consuming
query operations, so it's critical that you
understand how your end users will need to combine data with joins when you're
modeling the data. To quickly recap how joins work, let's consider these two tables. The orders table contains
information about each order placed in
an commerce company, and the customers table contains information about each customer. These two tables are related
to each other through the customer's ID and are part of a normalized model
that has data stored in separate tables to
reduce redundancy. To simplify the
process of finding information about an order and the customer who placed it, you can combine the data
from the orders and customers table using
a SQL JOIN statement. You'll select all the records from the orders table that's joined with the customer's table on the ID column from
the customers table, and the customer ID column
from the orders table. This way, the corresponding
customer information from the customers table
is combined with the orders information
based on the customer's ID. For example, for
these three orders that all have a
customer ID of one, you'll append the
customer information for the customer with ID of one. Then for this next order that
has a customer ID of two, you'll append the
customer information for the customer with ID
of two and so on. This type of join is
known as the inner join, where it combines data
from only the rows that share a matching customer
ID in both tables. To help you understand
why the join operation is one of the most
time-consuming query operations, let's go through
three common methods for implementing joins. Most database query
optimizers will use one of these methods when devising an execution plan
for join statement. The default method is
the nested loop join, which works like a
nested for loop. Starting with the first
row in the orders table, the database takes note
of the customer ID, then it scans every row in
the customers table and only retrieves the rows with
this matching customer ID. It combines the information
from this row of the orders table with
a retrieved rows of customer information. This is repeated for every
row in the orders table and the combined results
are returned at the end. The second method is the
index-based nested loop, which is a variation
of the first method. This method can be
used when an index exists for at least one
of the join attribute, which in this case, could
be the ID column in the customers table or the customer ID column
in the orders table. Let's assume an index exists for the ID column in the
customers table. You might have a B-tree
structure with a root node, internal nodes, and leaf nodes that looks
something like this. To execute this join, the database retrieves a first
row from the orders table, and then uses the ID
index to locate all rows in the customers table that
have a matching ID of one. From the root node, since one is less than 10, it'll go down to the
first internal node. Then since one is
less than three, it'll go down to the
first leaf node. Finally, it finds unique ID of one and its corresponding row addressed from the
customers table. The database will then retrieve
the data from this row in the customers table and join it with the first row
from the orders table. It will repeat this
for every row in the orders table and then
return the combined results. The last method is
the hash join method. This method uses a hash
function to map the rows from each table to buckets based on the value of the
join attribute, which is a customer's
ID in this example. With this method, the database first scans all rows
of the smaller table, which is the customers
table in this case, and sends each row to
a particular bucket. For example, we could have
a hash function that maps Row 1 from the customer's
table to the first bucket, Rows 2 and 3 to
the second bucket, and Row 4 to the last bucket. Then the database scans through the rows of the
larger orders table, sending each row to a bucket based on the same hash function. For example, it could
send Rows 1, 2, and 3 from the orders
table to the first bucket, Rows 4 and 5 to
the second bucket, then Row 6 to the last bucket. Finally, within each bucket, the database combines the
rows from the customers table and the orders table that
share matching customer IDs. In the first bucket here, since the row from
the customers table and the rows from
the orders table, all have a customer ID of one, the database would combine
all these into three rows. In the second bucket here, the database would combine the rows with the
customer ID of two, and then separately combine the rows with the
customer ID of three. Then finally, in
the third bucket, it'll combine the two rows that both have a customer ID of four. At the end, it'll
return the combined results.This method can be much faster because scanning
these smaller buckets in parallel can be much faster than scanning
the entire table. In all of these methods,
the join operation requires a database to scan a
considerable number of rows from each table, and that can occur
multiple times. That's what makes a
join operation one of the most time consuming
query operations. Aside from being able
to write efficient join queries yourself as
a data engineer, you should model and serve data to your end users in a way that enables them to easily
join the data when needed. The data model and schema you choose affects the number of joins or end users need to perform to get the
data that they want. In general, normalized schemas result in less
redundancy in your data, but require more complex join statements to
combine the data. Here are two schemas
you've seen in the first slab of
the specialization. The first is a
normalized schema of the input data that you
ingested in your pipeline. You then ran a glue op to
transform the data into the star schema before serving
it to the data analyst. Let's say the data
analyst is interested in computing the total number of products sold for each country. If you kept the data in its
original normalized schema, then the data analyst would need to join the
customers table with the orders table and then join the orders table with the
orders details table. On the other hand,
in the star schema, the data analyst would only
need to perform one join by combining the fact_orders table with the dim_locations table. Another alternative
is to combine the relevant attributes
into one big table. Then the data analyst won't need to perform
any joins at all. Don't worry if data
modeling is new to you. We'll get more into the
details of data modeling and the pros and cons of each
approach in the next course. But aside from
modeling your data, another challenge you'll likely encounter when working with joins is when two tables have a many to
many relationship. For example, the payments
and orders table shown here have a many
to many relationship. A payment can be associated
with many orders and an order can be paid
over several payments. Each payment is associated
with a customer, and let's say that
customer number 1 made five different payments. Each order is also
associated with a customer, and let's suppose that
customer number 1 made five different orders. You can join the
payments table and the orders table based on the customer number
to get this table. That sounds simple, right? The problem is that this table might not represent what
you think it should. At first glance, you might
think that this table shows the payment information and
their associated orders, but there's actually an error
in the join logic here. By joining the table on
the customer number, you are blindly joining
the payments table with the orders table without considering if a
given payment is correctly mapped to its
corresponding order. Because of this
error, every row in the payments table that's
associated with Customer 1 is mapped to every row
in the orders table that's also associated
with Customer 1. This creates five times five
or 25 rows in the output. Now, suppose that there are many other repeats in the
customer number column. This leads to a scenario
known as row explosion, where the query returns
more rows than expected. Row explosion can generate enough output rows to
consume a massive amount of database resources and can actually cause queries
to fail occasionally. Avoid this problem, make sure that you check your query plan to see if it correctly describes what you intend the join to do. If this query will
be run frequently, then consider adding a
table to your model that correctly maps a payment to its corresponding
order number. Understanding how joins or
process can help you design more efficient join
queries as well as properly model the data
for your end users. I hope you'll join me in the next video for a look
at aggregate queries.