You just learned a bit more
about how Apache Kafka works. Now I want to help
you understand how Amazon Kinesis Data Streams works before you
head into the lab. As you already know,
similar to Kafka, there are event producers
that push data to the stream and consumers which
read data from the stream. In the upcoming lab,
you'll be working with a Kinesis Data Stream as
your upstream source system. You won't be responsible for setting up the stream itself. However, you've also seen that you can
incorporate producers, consumers, and streams into other parts of your
data systems as well. Here I'd like to
walk you through some of the details of Kinesis that are relevant when you're in control of the entire system, which includes the producer, consumer, and the
stream in this case. Just like how in Kafka, producers send data to topics. When working with
Kinesis Data Streams, a producer pushes data
to a specific stream. One stream is made
up of many shards, which provide the units of
capacity for the stream. As you need to scale your
stream up to ingest more data, you need to add more
shards to the stream. In order to know how many
shards you'll need for your use case or when you'll need to increase the
number of shards, you'll need to know
the size and rate of write and read operations you're expecting
in your pipeline. Rite operations are when an event producer writes
data to the stream, and read operations are when downstream consumers read
data from the stream. Comes to capacity,
each chard can support up to five read
operations per second, and those five
operations can add up to a maximum total data read
rate of two megabytes/second. For writing data, a
producer can write up to 1,000 records
per second to a hard with a maximum
total data write rate at one megabyte/second. To determine the
number of shards you would need for a
specific use case, it would take some
analysis and some math, given the size and rate of read and write operations
that you expect. Sometimes it can be
hard to estimate the exact number of read
and write operations. For example, in a
brand new application. Or in other cases, the only thing you might know
for sure is that you expect the traffic in your application to vary dramatically over time, like on an e commerce platform or other public
facing applications. For those situations,
you can use Kinesis in on demand mode. On demand mode will
automatically manage the scaling of the shards
up or down as needed, and you are only charged
for what you use. This can be more convenient from an operational perspective when compared to the alternative, which is provisioned mode. With provision mode, you specified the number of
shards necessary for your application based on the expected write and
read request rate. And then it's up to you to add more shards or re
shard when needed. Provision mode might
be a good fit for your work if you have
predictable application traffic, or if you want to be able to control your costs
more carefully. When it comes to the data
moving through a stream, each data record that
is sent to the stream from a producer includes
a partition key, a sequence number, and the
data itself in the form of what's called a binary large
object or blob for short. When you are setting up the data producer for your system, you need to choose
a partition key. The partition key is then
used to determine which shard the data record
is placed into. Kinesis itself then assigns
a sequence number as each record is
written to maintain the order of the records
within the Shard. For example, let's say you
wanted to create a stream of transactions from an
e commerce platform. Then you might want to use the customer ID as
the partition key. In this case, all
transactions for a single customer could be
stored in the same shard. This would then
make it easier for downstream consumers to pool records related to
a single customer for aggregation and analysis. A producer puts
data into shards, and a consumer reads
data from Shards, and it is common to have multiple consumers reading
data from a Shard. By default, consumers share
a Shards read capacity, which is called shared fan out. This means that
the consumers are contending for read capacity. This can be an issue
for some use cases. To avoid running into
this capacity issue, you can set things up so that each consumer is able to read at the full two megabytes/second read capacity of the Shard, which is called
enhanced fan out. You can use managed services
such as AWS Lambda, Amazon Managed Service
for Apache Flink, and ADS Glue to process data stored in
Kinesis data streams, or you can write your own
custom consumers using the Amazon Kinesis
client library or KCL. You can also set things
up so that the output of one stream becomes
the input for another, which can allow you to build more complex real time
data processing workflows. Consumers can also send
data to other ADS services, like integrating with
Amazon Data Firehose to store data in Amazon three. It's important to remember too that Kinesis Data
Streams allows for multiple applications to work with the same stream
at the same time. Each one consuming the
data independently and sending that data downstream
to different systems. Next up, Joe will walk you through the details
of the upcoming lab. Then you'll be off to work on streaming adjetion
with Kinesis yourself. Good luck, and have fun.