In week one of this course, we looked
at streaming systems, including message queues and event streaming platforms,
from the perspective of these being source systems, where your data pipeline was on
the consumer end of these data sources. I also mentioned that depending on
the system you're working with, the actual source might just
be the event producer or multiple elements of a streaming system,
like multiple producers, brokers, and consumers could be upstream
of your ingestion system. In the conversation we just had
with the software engineer, the plan we came up with was for the
engineer to set up a kinesis data stream, and you'll be consumer of
messages from that stream. In this video, I'd like to talk a little
more about the details of message streams, and after that you'll be
on your way to the lab. And so as a quick reminder,
you'll have event streaming platforms and message queues as the two main
modalities of streaming ingestion. A message queue is essentially a buffer
used to deliver messages from an event producer to a consumer in
an asynchronous manner. Message queues typically operate on
a first in, first out or Fifo basis, meaning that the event consumer will
always read the oldest message in the queue first, and once a message is
consumed, it is deleted from the queue. Event streaming platforms,
on the other hand, function by storing messages in
a persistent way in an append-only log. The event router distributes messages
in the log to the subscribers, and it's possible to replay or
reprocess any messages in the log. You'll be using Amazon's Kinesis service
as your event streaming platform in the lab, but another widely
used platform is Apache Kafka. In this video, I'll use Kafka as
the example platform to talk through some of the details and draw parallels
with Kinesis where they exist. And then in the next video, Morgan will
go through kinesis in more detail. So by the end of this week you can have
some context on both of these solutions. So Apache Kafka is an open-source
event streaming platform, and while streaming platforms come in a number
of different flavors and varieties, the principles of how events are routed
and stored are similar across platforms. At a high level, event producers send or
push messages over the network to a Kafka cluster, which contains one or
more servers, also called brokers. Then event consumers read or
pull messages from that Kafka Cluster. Let's zoom in a bit on
the Kafka Cluster I have here. Within a Kafka cluster, message streams
are split up and routed into what are called topics you can think of a topic
like a category that holds a collection of related events, or maybe in another sense,
like a road to somewhere. And so messages line up in topics similar
to how rows of cars line up on different highways based on their destination. A topic could contain any type of
messages, for example, fraud alerts, customer orders, or
temperature readings from IoT devices. And it's a job of the producer to send
a message to its corresponding topic. Each topic has one or more partitions,
which are just logs containing ordered, immutable sequences of messages that
you continually add new messages to. Using the roadsysumware analogy,
partitions are like lanes on the highway. More lanes allow more cars
to pass through, and so each partition handles a subset of
messages as they are added to the topic. This allows for
more efficient traffic or message flow. And again, it's a job of the producer to decide on
which partition to send each message. The decision could be based on
a round-robin strategy, for example, or by computing the destination
partition based on the message key. With kinesis, all these concepts
are essentially the same, but instead of topics, you have streams,
and instead of partitions, you have what are called shards. Now, on the other end,
consumers are grouped, and each consumer group can
subscribe to one or more topics. The consumers of a group cooperate
together to consume the messages from all partitions of a given topic. Each partition can only be assigned to
a single consumer in the group, and each consumer consumes messages from
a different subset of partitions. When a producer publishes
a message to a topic partition, that message is delivered to one consumer
within each subscribing consumer group. Once a message is published to a topic,
the Kafka cluster retains that information for a configurable period of time,
whether or not the message was consumed. This allows consumers to replay and
reprocess messages as needed. In our conversation with the software
engineer, we learned that the user actions on the website are recorded as
messages in the web server log, and those messages will be
routed to a kinesis data stream. Similarly, those messages could
be routed to a Kafka topic, and you could consume them from there
by subscribing to that topic under a different set of circumstances. You could imagine a scenario where you
have direct access to monitor the web server log for new messages. Then you could treat the web
server log as the event producer. From there, events could be
ingested to a Kafka topic or a Kinesis stream as a first step
in your ingestion pipeline. You could also monitor database
activity through a process known as continuous change data capture,
or continuous CDC. By processing the database log, you can stream data changes into your
data pipeline to ensure that the data in your pipeline is synchronized with
the data updates in the source database. Up next, Morgan will walk you through the
details of Amazon Kinesis data streams, which we'll use as the streaming
ingestion tool in the upcoming lab. And after that, I'll be back to give
you a quick walkthrough of the last lab before you jump in to build your
own streaming ingestion solution.