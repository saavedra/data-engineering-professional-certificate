Welcome to week three. This week is all about dataops, but really
it includes elements of the software engineering and
data management undercurrents as well. As you learned in the previous course,
Dataops is a set of practices and cultural habits centered around
building robust data systems and delivering high quality data products. And DataOps really emerge from DevOps,
which is the set of practices and cultural habits that allow software
engineers to efficiently deliver and maintain high quality software products. This week, we're going to get into the
details of each of the three pillars of data ops, namely automation, observability
and monitoring, and incident response. Well, actually, to be fair, we're going to
spend more time looking at automation and observability and monitoring, and
not as much time with incident response. But that's not because incident
response is less important. It's just that incident response has
more to do with the cultural habits side of data ops, and it's harder to build practical exercises
around that in an online course. Anyhow, when it comes to automation, we'll be revisiting some of the ideas
covered in the previous course, like continuous integration and continuous
delivery or CI CD, and then zeroing in on the concept of infrastructure as
code, which is to say, writing code that when you run it deploys the resources
required to run your data pipeline. In the previous labs in these courses, you've gotten some experience spinning
up certain resources for your data infrastructure using the AWS console
on the job as a data engineer, however, it's becoming common practice to do this
deployment through infrastructure as code frameworks rather than manually spinning
up instances and installing software. And in fact, you already have some
experience with infrastructure as code in many of the labs you've already completed. We use infrastructure as code tools
like CloudFormation and terraform to set up the lab environment and deploy various
resources in your data pipelines. In the first lab this week,
you'll get hands on experience actually writing the code in terraform
to deploy your infrastructure. And after that, in the labs that follow,
you'll build on top of your infrastructure to include monitoring of your data quality
and other important observability metrics. Another exciting thing we have in
store for you this week is a set of interviews with industry experts
from the field of data ops. In these interviews, you'll be hearing
about the key aspects of data quality and data observability from people who
are actually building products to serve data engineers in these areas. So it's going to be an action packed
week of materials to kick off this week, we'll hear from one of my good friends,
Chris Berg, who is the co author of the DataOps
Manifesto and CEO at Data Kitchen, a popular data observability and
DataOps tool. And so the next video is
a conversation I had with Chris, where you'll hear how he defines dataops
and why he thinks this undercurrent is so important when building data products. Then after that, we'll look at the first
pillar of DataOps, which is automation.